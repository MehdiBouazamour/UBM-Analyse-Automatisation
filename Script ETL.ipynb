{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f1e449",
   "metadata": {},
   "source": [
    "**Script pour récuperer la data, la nettoyer et la placer dans un dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db909aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Création d'un client BigQuery\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configuration des paramètres pour chaque sport avec leurs endpoints et clés API spécifiques\n",
    "configs = {\n",
    "    \"basket\": {\n",
    "        \"table_id\": \"basket_orders\",\n",
    "        \"api_url\": os.environ.get(\"SHOPIFY_BASKET_ENDPOINT\"),\n",
    "        \"api_key\": os.environ.get(\"SHOPIFY_BASKET_API_KEY\"),\n",
    "        \"password\": os.environ.get(\"SHOPIFY_BASKET_PASSWORD\")\n",
    "    },\n",
    "    \"rugby\": {\n",
    "        \"table_id\": \"rugby_orders\",\n",
    "        \"api_url\": os.environ.get(\"SHOPIFY_RUGBY_ENDPOINT\"),\n",
    "        \"api_key\": os.environ.get(\"SHOPIFY_RUGBY_API_KEY\"),\n",
    "        \"password\": os.environ.get(\"SHOPIFY_RUGBY_PASSWORD\")\n",
    "    },\n",
    "    \"foot\": {\n",
    "        \"table_id\": \"foot_orders\",\n",
    "        \"api_url\": os.environ.get(\"SHOPIFY_FOOT_ENDPOINT\"),\n",
    "        \"api_key\": os.environ.get(\"SHOPIFY_FOOT_API_KEY\"),\n",
    "        \"password\": os.environ.get(\"SHOPIFY_FOOT_PASSWORD\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fonction pour extraire une valeur d'un dictionnaire contenu dans une série Pandas\n",
    "def extract_from_dict(col, key):\n",
    "    return col.apply(lambda x: x.get(key) if isinstance(x, dict) else None)\n",
    "\n",
    "# Fonction pour extraire une chaîne de valeurs à partir d'une liste de dictionnaires dans une série Pandas\n",
    "def extract_from_list_of_dicts(col, key, separator=\", \"):\n",
    "    return col.apply(lambda x: separator.join([str(sub[key]) for sub in x if key in sub]) if isinstance(x, list) else None)\n",
    "\n",
    "# Fonction pour sommer tous les montants de réduction à partir de la liste des codes de réduction\n",
    "def sum_discounts(col):\n",
    "    return col.apply(lambda x: sum([float(d['amount']) for d in x]) if isinstance(x, list) else 0)\n",
    "\n",
    "# Fonction pour supprimer les pictogrammes et caractères indésirables d'une chaîne de texte\n",
    "def remove_pictograms(text):\n",
    "    text = re.sub(r'\\(X\\d+\\)', '', text)  \n",
    "    text = re.sub(r'\\+', '', text)        \n",
    "    text = re.sub(r'/\\s*', '', text)      \n",
    "    emoji_reg = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE) \n",
    "    text = emoji_reg.sub(r'', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Fonction pour extraire les tailles des articles d'une chaîne de texte\n",
    "def extract_sizes(lineitem_name, predefined_sizes):\n",
    "    if isinstance(lineitem_name, str):\n",
    "        for size in predefined_sizes:\n",
    "            if re.search(r'\\b{}\\b'.format(re.escape(size)), lineitem_name, flags=re.IGNORECASE):\n",
    "                return size\n",
    "    return None\n",
    "# Prédéfinition des tailles standard pour la recherche dans les noms des articles\n",
    "predefined_sizes = ['Standard', 'L', '13/14 ans', '11/12 ans', '5/6 ans', '9/10 ans', '7/8 ans', 'XL', 'S', 'M', 'XS', '2XL', 'XXL', '3XL', '4XL', '5XL', '3/4 ans', '4 ans', '14 ans', '10 ans', '12 ans', '8 ans', '6 ans', 'Taille unique']\n",
    "\n",
    "# Fonction pour décomposer la colonne 'line_items' en plusieurs lignes pour chaque commande\n",
    "def unpack_items(df):\n",
    "    items = df['line_items'].apply(lambda x: [{'quantity': item['quantity'], \n",
    "                                                'price': item['price'], \n",
    "                                                'title': item['title'],\n",
    "                                                'variant_title': item.get('variant_title', 'Unknown')}  # Use .get() with default\n",
    "                                                for item in x] if isinstance(x, list) else None)\n",
    "    rows = []\n",
    "    for i, row in enumerate(items):\n",
    "        if row:\n",
    "            for item in row:\n",
    "                new_row = df.iloc[i].to_dict()\n",
    "                new_row.update(item)\n",
    "                rows.append(new_row)\n",
    "        else:\n",
    "            new_row = df.iloc[i].to_dict()\n",
    "            new_row.update({'quantity': None, 'price': None, 'title': None, 'variant_title': 'Unknown'})  # Provide default value here too\n",
    "            rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Dictionnaire de correspondance des départements français aux régions\n",
    "departments_to_regions = {\n",
    "    '01': 'Auvergne-Rhône-Alpes',\n",
    "    '02': 'Hauts-de-France',\n",
    "    '03': 'Auvergne-Rhône-Alpes',\n",
    "    '04': 'Provence-Alpes-Côte dAzur',\n",
    "    '05': 'Provence-Alpes-Côte dAzur',\n",
    "    '06': 'Provence-Alpes-Côte dAzur',\n",
    "    '07': 'Auvergne-Rhône-Alpes',\n",
    "    '08': 'Grand Est',\n",
    "    '09': 'Occitanie',\n",
    "    '10': 'Grand Est',\n",
    "    '11': 'Occitanie',\n",
    "    '12': 'Occitanie',\n",
    "    '13': 'Provence-Alpes-Côte dAzur',\n",
    "    '14': 'Normandie',\n",
    "    '15': 'Auvergne-Rhône-Alpes',\n",
    "    '16': 'Nouvelle-Aquitaine',\n",
    "    '17': 'Nouvelle-Aquitaine',\n",
    "    '18': 'Centre-Val de Loire',\n",
    "    '19': 'Nouvelle-Aquitaine',\n",
    "    '20': 'Corse',\n",
    "    '21': 'Bourgogne-Franche-Comté',\n",
    "    '22': 'Bretagne',\n",
    "    '23': 'Nouvelle-Aquitaine',\n",
    "    '24': 'Nouvelle-Aquitaine',\n",
    "    '25': 'Bourgogne-Franche-Comté',\n",
    "    '26': 'Auvergne-Rhône-Alpes',\n",
    "    '27': 'Normandie',\n",
    "    '28': 'Centre-Val de Loire',\n",
    "    '29': 'Bretagne',\n",
    "    '30': 'Occitanie',\n",
    "    '31': 'Occitanie',\n",
    "    '32': 'Occitanie',\n",
    "    '33': 'Nouvelle-Aquitaine',\n",
    "    '34': 'Occitanie',\n",
    "    '35': 'Bretagne',\n",
    "    '36': 'Centre-Val de Loire',\n",
    "    '37': 'Centre-Val de Loire',\n",
    "    '38': 'Auvergne-Rhône-Alpes',\n",
    "    '39': 'Bourgogne-Franche-Comté',\n",
    "    '40': 'Nouvelle-Aquitaine',\n",
    "    '41': 'Centre-Val de Loire',\n",
    "    '42': 'Auvergne-Rhône-Alpes',\n",
    "    '43': 'Auvergne-Rhône-Alpes',\n",
    "    '44': 'Pays de la Loire',\n",
    "    '45': 'Centre-Val de Loire',\n",
    "    '46': 'Occitanie',\n",
    "    '47': 'Nouvelle-Aquitaine',\n",
    "    '48': 'Occitanie',\n",
    "    '49': 'Pays de la Loire',\n",
    "    '50': 'Normandie',\n",
    "    '51': 'Grand Est',\n",
    "    '52': 'Grand Est',\n",
    "    '53': 'Pays de la Loire',\n",
    "    '54': 'Grand Est',\n",
    "    '55': 'Grand Est',\n",
    "    '56': 'Bretagne',\n",
    "    '57': 'Grand Est',\n",
    "    '58': 'Bourgogne-Franche-Comté',\n",
    "    '59': 'Hauts-de-France',\n",
    "    '60': 'Hauts-de-France',\n",
    "    '61': 'Normandie',\n",
    "    '62': 'Hauts-de-France',\n",
    "    '63': 'Auvergne-Rhône-Alpes',\n",
    "    '64': 'Nouvelle-Aquitaine',\n",
    "    '65': 'Occitanie',\n",
    "    '66': 'Occitanie',\n",
    "    '67': 'Grand Est',\n",
    "    '68': 'Grand Est',\n",
    "    '69': 'Auvergne-Rhône-Alpes',\n",
    "    '70': 'Bourgogne-Franche-Comté',\n",
    "    '71': 'Bourgogne-Franche-Comté',\n",
    "    '72': 'Pays de la Loire',\n",
    "    '73': 'Auvergne-Rhône-Alpes',\n",
    "    '74': 'Auvergne-Rhône-Alpes',\n",
    "    '75': 'Île-de-France',\n",
    "    '76': 'Normandie',\n",
    "    '77': 'Île-de-France',\n",
    "    '78': 'Île-de-France',\n",
    "    '79': 'Nouvelle-Aquitaine',\n",
    "    '80': 'Hauts-de-France',\n",
    "    '81': 'Occitanie',\n",
    "    '82': 'Occitanie',\n",
    "    '83': 'Provence-Alpes-Côte dAzur',\n",
    "    '84': 'Provence-Alpes-Côte dAzur',\n",
    "    '85': 'Pays de la Loire',\n",
    "    '86': 'Nouvelle-Aquitaine',\n",
    "    '87': 'Nouvelle-Aquitaine',\n",
    "    '88': 'Grand Est',\n",
    "    '89': 'Bourgogne-Franche-Comté',\n",
    "    '90': 'Bourgogne-Franche-Comté',\n",
    "    '91': 'Île-de-France',\n",
    "    '92': 'Île-de-France',\n",
    "    '93': 'Île-de-France',\n",
    "    '94': 'Île-de-France',\n",
    "    '95': 'Île-de-France',\n",
    "    '97': 'Outre mer',\n",
    "    '0': 'Autres pays',\n",
    "}\n",
    "\n",
    "# Liste des colonnes requises pour la base de données BigQuery\n",
    "columns_required = [\n",
    "    'id', 'created_at', 'closed_at', 'order_number', 'current_subtotal_price', 'current_total_discounts',\n",
    "    'current_total_price', 'current_total_tax', 'email', 'source_name', 'name', 'user_id',\n",
    "    'subtotal_price', 'total_price', 'discount_code', 'discount_amount', 'tags',\n",
    "    'shipping_amount', 'shipping_address1', 'shipping_zip', 'shipping_country_code',\n",
    "    'quantity', 'price', 'title', 'region', 'sizes', 'zip', 'buyer_accepts_marketing'\n",
    "]\n",
    "\n",
    "# Fonction pour charger les données dans BigQuery\n",
    "def load_data_to_bigquery(df, dataset_id, table_id):\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"_id_\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"closed_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"order_number\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"current_subtotal_price\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"current_total_discounts\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"current_total_price\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"current_total_tax\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"email\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"source_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"user_id\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"subtotal_price\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"total_price\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"discount_code\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"discount_amount\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"tags\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"shipping_amount\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"shipping_address1\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"shipping_zip\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"shipping_country_code\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"quantity\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"price\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"region\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"sizes\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"zip\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"buyer_accepts_marketing\", \"BOOLEAN\"),\n",
    "    ]\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=\"WRITE_APPEND\")\n",
    "    job = client.load_table_from_dataframe(df, f\"{client.project}.{dataset_id}.{table_id}\", job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Loaded {df.shape[0]} rows into {dataset_id}:{table_id}\")\n",
    "\n",
    "# Fonction pour récupérer et charger les données pour un sport spécifique\n",
    "def fetch_and_load_data_for_sport(sport_key):\n",
    "    config = configs[sport_key]\n",
    "    table_id = f\"elated-bison-419709.bonmaillot.{config['table_id']}\"\n",
    "    \n",
    "    # Fetch the latest close timestamp from BigQuery\n",
    "    query = f'SELECT MAX(closed_at) AS last_close FROM `{table_id}`'\n",
    "    result = client.query(query).to_dataframe()\n",
    "    last_close = result['last_close'][0] if not result.empty else None\n",
    "    print(f\"Last close for {sport_key} was on: {last_close}\")\n",
    "\n",
    "    # Setup API request\n",
    "    params = {'limit': 250, 'status': 'any'}\n",
    "    if last_close:\n",
    "        formatted_last_close = (last_close + timedelta(seconds=1)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        url = f\"{config['api_url']}?limit={params['limit']}&status={params['status']}&created_at_min={formatted_last_close}\"\n",
    "    else:\n",
    "        url = f\"{config['api_url']}?limit={params['limit']}&status={params['status']}\"\n",
    "\n",
    "    all_orders = []\n",
    "    while url:\n",
    "        response = requests.get(url, auth=(config['api_key'], config['password']))\n",
    "        if response.status_code == 429:\n",
    "            time.sleep(10) # Gestion de la limitation de taux\n",
    "            continue\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data for {sport_key}: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "        data = response.json()\n",
    "        orders = data.get('orders', [])\n",
    "        all_orders.extend(orders)\n",
    "        print(f\"Fetched {len(orders)} orders for {sport_key}.\")\n",
    "        url = response.links.get('next', {}).get('url')\n",
    "\n",
    "    if all_orders:\n",
    "        df = pd.DataFrame(all_orders)\n",
    "        df = process_dataframe(df)\n",
    "        load_data_to_bigquery(df, 'bonmaillot', config['table_id'])\n",
    "    else:\n",
    "        print(f\"No new orders to process for {sport_key}.\")\n",
    "\n",
    "# Fonction principale pour traiter les données de dataframe\n",
    "def process_dataframe(df):\n",
    "    date_columns = ['created_at', 'closed_at']  \n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    df = unpack_items(df)\n",
    "    df['sizes'] = df['variant_title'].apply(lambda x: extract_sizes(x, predefined_sizes))\n",
    "    df['discount_amount'] = sum_discounts(df['discount_codes'])\n",
    "    df['shipping_amount'] = extract_from_dict(df['total_shipping_price_set'].apply(lambda x: x.get('shop_money') if isinstance(x, dict) else None), 'amount')\n",
    "    df['discount_code'] = extract_from_list_of_dicts(df['discount_codes'], 'code')\n",
    "    df['shipping_address1'] = extract_from_dict(df['shipping_address'], 'address1')\n",
    "    df['shipping_zip'] = extract_from_dict(df['shipping_address'], 'zip')\n",
    "    df['shipping_country_code'] = extract_from_dict(df['shipping_address'], 'country_code')\n",
    "    df['shipping_zip'] = df['shipping_zip'].astype(str)\n",
    "    df['region'] = df.apply(lambda x: departments_to_regions.get(x['shipping_zip'][:2]) if x['shipping_country_code'] == 'FR' else None, axis=1)\n",
    "    df['title'] = df['title'].apply(remove_pictograms)\n",
    "    df['name'] = extract_from_dict(df['billing_address'], 'name')\n",
    "    df['user_id'] = extract_from_dict(df['customer'], 'id')\n",
    "    df['zip'] = df.apply(lambda x: x['shipping_zip'][:2] if x['shipping_country_code'] == 'FR' else None, axis=1)\n",
    "    df['region'] = df['zip'].map(departments_to_regions)\n",
    "    df['zip'] = pd.to_numeric(df['zip'], errors='coerce').fillna(0).astype(int)\n",
    "    float_columns = [\n",
    "        'current_subtotal_price', 'current_total_discounts', 'current_total_price',\n",
    "        'current_total_tax', 'subtotal_price', 'total_price', 'discount_amount', \n",
    "        'shipping_amount', 'price', 'user_id'\n",
    "    ]\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df[columns_required]\n",
    "    df = df.rename(columns={\"id\": \"_id_\"})\n",
    "    return df\n",
    "\n",
    "# Boucle sur chaque configuration de sport pour lancer la récupération et le chargement des données\n",
    "for sport in configs.keys():\n",
    "    fetch_and_load_data_for_sport(sport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
